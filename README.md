# ğŸ§  Multimodal Fashion & Context Retrieval

## ğŸ“Œ Overview

This project implements a **multimodal fashion image retrieval system** that retrieves relevant fashion images based on **natural language queries**.
The system understands **clothing attributes, visual appearance, and contextual intent** (e.g., formal vs casual), enabling more accurate retrieval than keyword-based search.

---

## ğŸ“¸ Retrieval Results

### 1.Attribute-specific Query
**1.Query:** A person in a bright yellow raincoat  
![Yellow Raincoat](screenshots/yellow_raincoat.png)

---

### 2.Contextual Query
**Query:** Professional business attire inside a modern office  
![Formal Office](screenshots/formal_office.png)

---

### 3.Complex Semantic:
**Query:** Someone wearing a blue shirt sitting on a park bench.
![Complex](screenshots/blue_shirt_park_bench.png)

---
### 4.Style Inference
**Query:** Casual weekend outfit for a city walk  
![Casual City](screenshots/casual_city.png)

---

### 5.Compositional Query
**Query:** A red tie and a white shirt in a formal setting  
![Red Tie](screenshots/red_tie_white_shirt.png)



## ğŸ§© Problem Statement

The goal is to build an intelligent search engine that:

* **Input:** A natural language fashion query (e.g., *â€œA red tie and a white shirt in a formal settingâ€*)
* **Output:** A ranked list of fashion images that best match the query


---
## ğŸ’¡ Key Ideas & Approach

### Baseline

* Used **CLIP (Contrastive Languageâ€“Image Pretraining)** for zero-shot imageâ€“text alignment

### Improvements Beyond Vanilla CLIP

* **Attribute-aware indexing**

  * Color distribution extracted from images (HSV histograms)
  * Fashion attributes loaded from Fashionpedia annotations
* **Soft context-aware re-ranking**

  * Lightweight intent extraction from query (e.g., â€œformalâ€, â€œcasualâ€)
  * Used only for *soft boosting*, not hard filtering


---

## ğŸ—ï¸ Architecture / System Overview

```
Text Query
   â”‚
   â–¼
CLIP Text Encoder
   â”‚
   â–¼
FAISS Vector Search  â”€â”€â–º Top-N Candidate Images
   â”‚
   â–¼
Soft Re-ranking (Context + Color)
   â”‚
   â–¼
Final Ranked Images
```



## ğŸ“Š Dataset

* **Dataset Used**: Fashionpedia (Validation/Test images)
* **Why Fashionpedia**:

  * Rich fashion diversity
  * Fine-grained attribute annotations
  * Suitable for fashion-centric reasoning
* **Usage**:

  * Images are used **only for indexing**, not supervised training
  * Annotations are used as **weak signals** for attribute awareness
* **Limitations**:

  * Context labels (e.g., *office*, *park*) are not explicit
  * Bias toward runway / editorial images

---

## ğŸ—‚ï¸ Project Structure

```
fashion-context-search/
â”‚
â”œâ”€â”€ indexer/
â”‚   â”œâ”€â”€ clip_encoder.py        # CLIP image embeddings
â”‚   â”œâ”€â”€ color_extractor.py     # Color histogram features
â”‚   â”œâ”€â”€ attribute_encoder.py  # Fashionpedia attribute loader
â”‚   â””â”€â”€ build_index.py         # FAISS index builder
â”‚
â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ query_parser.py        # Lightweight intent extraction
â”‚   â”œâ”€â”€ search.py              # Retrieval + re-ranking logic
â”‚   â””â”€â”€ show_results.py        # Visualization script
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/                # Fashion images (not pushed)
â”‚   â””â”€â”€ annotations.json
â”‚
â”œâ”€â”€ screenshots/               # Retrieval results
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation & Setup

```bash
# Create virtual environment
python -m venv venv
source venv/Scripts/activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Build Image Index

```bash
cd indexer
python build_index.py
```

### 2ï¸âƒ£ Run Retrieval & Visualization

```bash
cd retriever
python show_results.py
```

---

## ğŸ§ª Evaluation Queries

The system is evaluated using the following prompts:

* **Attribute Specific**:
  *â€œA person in a bright yellow raincoat.â€*

* **Contextual / Place**:
  *â€œProfessional business attire inside a modern office.â€*

* **Complex Semantic**:
  *â€œSomeone wearing a blue shirt sitting on a park bench.â€*

* **Style Inference**:
  *â€œCasual weekend outfit for a city walk.â€*

* **Compositional**:
  *â€œA red tie and a white shirt in a formal setting.â€*

---

## âš–ï¸ Design Choices & Tradeoffs

### Why CLIP?

* Strong zero-shot visionâ€“language alignment
* No training required
* Generalizes well to unseen queries

### Why FAISS?

* Scales efficiently to millions of vectors
* Industry-standard vector retrieval

### Why Soft Re-ranking?

* Avoids brittle hard rules
* Improves precision without sacrificing recall

### What Was Not Done

* No supervised fine-tuning (to preserve zero-shot nature)
* No heavy scene classifiers (kept solution lightweight)

---
## ğŸ“ˆ Scalability

* Image embeddings are computed **once**
* FAISS supports **million-scale datasets**
* Retrieval time grows sub-linearly

This system can scale to **1M+ images** with minimal changes.

---

## ğŸš€ Zero-Shot Capability

**Zero-shot** means the system can handle queries it has **never seen before**.

* CLIP enables unseen color, style, and clothing combinations
* No hard-coded labels or rules
* Limitations arise when context is visually ambiguous

---

## âš ï¸ Limitations

* Context (e.g., *park*, *office*) inferred indirectly
* Fashionpedia images are biased toward editorial scenes
* Some compositional queries remain visually ambiguous

---

## ğŸ”® Future Work

* Learnable re-ranking using contrastive loss
* Scene classifiers for explicit context detection
* Weather & location-aware embeddings
* Better compositional reasoning for multi-garment queries


---

## ğŸ‘¤ Author

**Antara Arkasali**
AI / ML Enthusiast
GitHub: [https://github.com/antara483](https://github.com/antara483)

---





